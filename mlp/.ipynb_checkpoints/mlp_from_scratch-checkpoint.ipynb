{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"Desktop/train-data.csv\", delimiter=',')\n",
    "all_ones = np.where(data[:,-1] == 1)[0]\n",
    "all_zeros = np.where(data[:,-1] == 0)[0]\n",
    "one_indices = np.random.choice(all_ones, 5914, replace=False)\n",
    "new_ones = np.take(data, one_indices, axis=0)\n",
    "new_zeros = np.take(data, all_zeros, axis=0)\n",
    "data = np.concatenate((new_ones, new_zeros))\n",
    "np.random.shuffle(data)\n",
    "train_X = data[:9000,:-1]\n",
    "train_X = (train_X - np.min(train_X, axis=0)) / (np.max(train_X, axis=0) - np.min(train_X, axis=0))\n",
    "val_X = data[9000:,:-1]\n",
    "val_X = (val_X - np.min(val_X, axis=0)) / (np.max(val_X, axis=0) - np.min(val_X, axis=0))\n",
    "train_y = data[:9000,-1:].astype(int)\n",
    "val_y = data[9000:,-1:].astype(int)\n",
    "test = np.genfromtxt(\"Desktop/test-data.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        self.weights = np.random.uniform(-0.5, 0.5, (in_channel, out_channel))\n",
    "        self.bias = np.random.uniform(-0.5, 0.5, (1, out_channel))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        self.output = X.dot(self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, running_grad, learning_rate):\n",
    "        bias_error = np.sum(running_grad, axis=0, keepdims=True)\n",
    "        weight_error = self.input.T.dot(running_grad)\n",
    "        running_grad = running_grad.dot(self.weights.T)\n",
    "        \n",
    "        self.bias -= learning_rate * bias_error\n",
    "        self.weights -= learning_rate * weight_error\n",
    "        return running_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, activation, activation_grad):\n",
    "        self.activation = activation\n",
    "        self.activation_grad = activation_grad\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, running_grad):\n",
    "        return self.activation_grad(self.input) * running_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_fn:\n",
    "    def __init__(self, loss_fn, loss_fn_grad):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_fn_grad = loss_fn_grad\n",
    "    \n",
    "    def forward(self, pred, label):\n",
    "        return self.loss_fn(pred, label)\n",
    "    \n",
    "    def backward(self, pred, label):\n",
    "        return self.loss_fn_grad(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "def relu_grad(X):\n",
    "    return (X > 0) * 1\n",
    "\n",
    "def mse(pred, true):\n",
    "    return np.mean((pred-true)**2)\n",
    "def mse_grad(pred, true):\n",
    "    return 2*(pred-true)/true.shape[0]\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def sigmoid_grad(X):\n",
    "    sig = sigmoid(X)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def log_loss(pred, true):\n",
    "    return -(1/pred.shape[0])*(true.T.dot(np.log(pred)) + (1 - true).T.dot(np.log(1-pred + 1e-7)))[0][0]\n",
    "\n",
    "def log_loss_grad(pred, true):\n",
    "    return -(((true+1e-7)/(pred+1e-7)) - ((1-true+1e-7)/(1-pred+1e-7))) / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        self.layer1 = Layer(in_channel, 400)\n",
    "        self.act1 = Activation(relu, relu_grad)\n",
    "        self.layer2 = Layer(400, 200)\n",
    "        self.act2 = Activation(relu, relu_grad)\n",
    "        self.layer3 = Layer(200, 100)\n",
    "        self.act3 = Activation(relu, relu_grad)\n",
    "        self.layer4 = Layer(100, out_channel)\n",
    "        self.act4 = Activation(sigmoid, sigmoid_grad)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.layer1.forward(X)\n",
    "        X = self.act1.forward(X)\n",
    "        X = self.layer2.forward(X)\n",
    "        X = self.act2.forward(X)\n",
    "        X = self.layer3.forward(X)\n",
    "        X = self.act3.forward(X)\n",
    "        X = self.layer4.forward(X)\n",
    "        X = self.act4.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, running_grad, learning_rate):\n",
    "        running_grad = self.act4.backward(running_grad)\n",
    "        running_grad = self.layer4.backward(running_grad, learning_rate)\n",
    "        running_grad = self.act3.backward(running_grad)\n",
    "        running_grad = self.layer3.backward(running_grad, learning_rate)\n",
    "        running_grad = self.act2.backward(running_grad)\n",
    "        running_grad = self.layer2.backward(running_grad, learning_rate)\n",
    "        running_grad = self.act1.backward(running_grad)\n",
    "        running_grad = self.layer1.backward(running_grad, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, loss_fn, train_data, train_label, val_data, val_label, epoch, learning_rate):\n",
    "    for i in range(epoch):\n",
    "        pred = model.forward(train_data)\n",
    "        loss = loss_fn.forward(pred, train_label)\n",
    "        \n",
    "        running_grad = loss_fn.backward(pred, train_label)\n",
    "        model.backward(running_grad, learning_rate)\n",
    "        \n",
    "        val_pred = model.forward(val_data)\n",
    "        val_loss = loss_fn.forward(val_pred, val_label)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{i + 1}]: Train Loss: {loss} Accuracy: {np.mean(1*(pred > 0.5) == train_label)}\")\n",
    "            print(f\"Epoch [{i + 1}]: Val Loss: {val_loss} Accuracy: {np.mean(1*(val_pred > 0.5) == val_label)}\")\n",
    "    print(f\"Epoch [{i + 1}]: Train Loss: {loss} Accuracy: {np.mean(1*(pred > 0.5) == train_label)}\")\n",
    "    print(f\"Epoch [{i + 1}]: Val Loss: {val_loss} Accuracy: {np.mean(1*(val_pred > 0.5) == val_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = Loss_fn(log_loss, log_loss_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]: Train Loss: 0.5472244704373752 Accuracy: 0.703\n",
      "Epoch [1]: Val Loss: 0.5490886825261129 Accuracy: 0.7029702970297029\n",
      "Epoch [51]: Train Loss: 0.524418681442739 Accuracy: 0.7166666666666667\n",
      "Epoch [51]: Val Loss: 0.5284189597832333 Accuracy: 0.708981612446959\n",
      "Epoch [101]: Train Loss: 0.5238656110665451 Accuracy: 0.7168888888888889\n",
      "Epoch [101]: Val Loss: 0.5279401013386511 Accuracy: 0.708981612446959\n",
      "Epoch [151]: Train Loss: 0.5233687311374036 Accuracy: 0.7168888888888889\n",
      "Epoch [151]: Val Loss: 0.5276151493355175 Accuracy: 0.7086280056577087\n",
      "Epoch [201]: Train Loss: 0.5229023267132719 Accuracy: 0.7175555555555555\n",
      "Epoch [201]: Val Loss: 0.5273039080408853 Accuracy: 0.7079207920792079\n",
      "Epoch [251]: Train Loss: 0.5224254891615868 Accuracy: 0.7187777777777777\n",
      "Epoch [251]: Val Loss: 0.5269903066488825 Accuracy: 0.7082743988684582\n",
      "Epoch [300]: Train Loss: 0.5220096790470193 Accuracy: 0.7185555555555555\n",
      "Epoch [300]: Val Loss: 0.5267049743262917 Accuracy: 0.7086280056577087\n"
     ]
    }
   ],
   "source": [
    "train_fn(model, loss_fn, train_X, train_y, val_X, val_y, 300, 0.0042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (test - np.min(test, axis=0)) / (np.max(test, axis=0) - np.min(test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.forward(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = 1*(predictions > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"predictions.csv\", pred,\n",
    "              delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
